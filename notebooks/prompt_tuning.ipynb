{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to login to wandb first using the cli:\n",
    "```\n",
    "wandb login\n",
    "<paste wandb api key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:fybbp6yo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prompt_tuning</strong> at: <a href='https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods/runs/fybbp6yo' target=\"_blank\">https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods/runs/fybbp6yo</a><br/> View project at: <a href='https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods' target=\"_blank\">https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240815_160037-fybbp6yo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:fybbp6yo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/finetune-llms/notebooks/wandb/run-20240815_160049-7niw8c7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods/runs/7niw8c7p' target=\"_blank\">prompt_tuning</a></strong> to <a href='https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods' target=\"_blank\">https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods/runs/7niw8c7p' target=\"_blank\">https://wandb.ai/jdgallegoq-wandb/prompt_learning_methods/runs/7niw8c7p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize project on wandb\n",
    "wandb.init(project='prompt_learning_methods', name='prompt_tuning')\n",
    "\n",
    "# set seed for reproducibility\n",
    "seed = 0\n",
    "\n",
    "# Hugguingface endpoints\n",
    "device = 'cuda'\n",
    "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "dataset_name = \"twitter_complaints\"\n",
    "\n",
    "# Dataset\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "\n",
    "# Model hyperparams\n",
    "max_length = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you already accepted access for the repo of this model `mistralai/Mistral-7B-v0.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6976eee39044d3bd50a6d2e8d0efd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc496b328da44ef89e3e54bf7bab6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/266k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6177032fe4484f48af5bf2a506c90048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cde30ada1274a2d8cb17b256f3f7794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unlabeled', 'complaint', 'no complaint']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74de50758bc64d8cb41b5647f0a4de61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a38ae2b1b42474cb6a9d04bb876983c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Tweet text', 'ID', 'Label', 'text_label'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Tweet text', 'ID', 'Label', 'text_label'],\n",
      "        num_rows: 3399\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tweet text': '@HMRCcustomers No this is my first job',\n",
       " 'ID': 0,\n",
       " 'Label': 2,\n",
       " 'text_label': 'no complaint'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "print(classes)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 33, 1: 17})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dataset[\"train\"][\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ea78fc0f65475a8253c093a8d26673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa67ecba3e44b92bb4b3ddabc42c789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acf9d1ed29743db91d3f7dbba63889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8084d374365c416395b733a30fcdd03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_max_length=4\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
    "print(f\"{target_max_length=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x}\\nLabel : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False)  # don't add bos token because we concatenate with inputs\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536b564f197e473188c47680667e55a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a58024e51904640b549612d3a4d1684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     1,   320, 12523,  2245,   714,  3194,\n",
       "          10198,   264,  8428,  4449,  1508, 28707, 28723,  1115, 28748, 28754,\n",
       "           1981, 28765, 28727, 28743, 28768,  7209, 28718,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     1,   320, 12523,  2245,   714,   802,\n",
       "          28796, 13773, 28796,  6316, 28790,   315,   776, 24385,   264,  5401,\n",
       "           3667,  3339,   297,  9231,  3217,  1286,  6247,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     1,   320, 12523,  2245,   714,   802, 28796,\n",
       "            269,  5157, 11782, 28730, 28743,   492, 28705,   708,  1982,   297,\n",
       "           6287,   287,  2698,  1101,   349,   378, 13873, 28723,    13,  4565,\n",
       "            714, 28705, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     1,   320, 12523,  2245,   714,   802,  9419,  1851,\n",
       "          28730, 28743,  8340,  4449,  1508, 28707, 28723,  1115, 28748,  2443,\n",
       "          23537, 28774, 28749, 28718, 28770, 28726, 28749,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              1,   320, 12523,  2245,   714,   802, 28798,  2180,  7627, 28738,\n",
       "           7915,  5625,   643,  7416,   298,   390,  1162, 28723,   995,   625,\n",
       "            680, 20497,  1012,   879, 28705, 29137, 29137,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              1,   320, 12523,  2245,   714,   802,  5760, 12764,  9452,   544,\n",
       "            302,   264,  5302,   315,   541, 28742, 28707,  5789,   298,   586,\n",
       "           6258, 21391,  3681,   562, 11683,   624,  3791,    13,  4565,   714,\n",
       "          28705,   708, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     1,   320, 12523,  2245,   714,   802, 28743,  8340,\n",
       "          25614,   802, 28757, 13167,   802,  1209, 24957,  1523, 28743,  4585,\n",
       "           1984,  6303,  2081,  2423,  6635,   659,   750, 16831,   304,  8939,\n",
       "          28725,   562,   767,   684,   272, 18990,  2081, 28804,    13,  4565,\n",
       "            714, 28705, 22105,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     1,   320, 12523,  2245,\n",
       "            714,   802, 28769,  5412,  3650,   586, 25546,   622,   459,  1914,\n",
       "            528,   298,  4987,  3181,  3519,   378,   865, 25831,  1326,  4042,\n",
       "           7421,   422, 20567, 28713,  2922,   422, 17722, 11538,    13,  4565,\n",
       "            714, 28705, 22105,     2]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,   708, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 22105,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 22105,     2]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")\n",
    "\n",
    "train_dataset = preprocessed_dataset[\"train\"]\n",
    "eval_dataset = preprocessed_dataset[\"train\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column}: {x}\\nLabel\" for x in examples[text_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id]*(\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19ad960d5a14b4fafeb0a852cdfa88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     1,   320, 12523,  2245, 28747,   802, 28760,\n",
       "            324, 14233,  2328,  8868,   354, 10313,   586,  7416,  2169,   395,\n",
       "            272,  4908,  8147,  1309,   356,   378, 28808, 11936, 28723, 22747,\n",
       "          28723,   675, 28748, 28710, 28737, 28734, 11788, 28744, 28762, 28779,\n",
       "          28779, 28750,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     1,   320,\n",
       "          12523,  2245, 28747,   334,   855,   288,   582,   356,   422,  3836,\n",
       "           3957,  1829, 19653, 28808,   415,   905,   590,  3088,   354,  1167,\n",
       "           4370,   568,  1371,   272,  8710,   472, 28705, 29137, 29137, 29274,\n",
       "          30155, 29096,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     1,   320, 12523,  2245, 28747, 15359,   802,\n",
       "          21270, 28713, 17302, 28725,   349,   378,  1055, 20775, 28713,  4920,\n",
       "            298,   865,  2405,   264, 20106,   513,   272,  2515,   349,  9683,\n",
       "          28724, 28804,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     1,   320, 12523,  2245, 28747,   802, 28798,  9399,\n",
       "          24855,  5156,   315, 28809, 28719, 13903,  1101,   335,   272,  1537,\n",
       "          28735,   349,   272,   981,   291,   529, 17381,  3372,   511,   368,\n",
       "           3091,   369,  8623, 28733,  8820,  6835,   460, 17381, 28878,  4449,\n",
       "           1508, 28707, 28723,  1115, 28748,   299, 17580, 28728, 28754,  2252,\n",
       "          28740, 28716,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "            320, 12523,  2245, 28747,   802, 13052,   795,   602,  7261,  6504,\n",
       "           8196,  1101,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     1,   320, 12523,  2245, 28747,   802,\n",
       "          28719,   641,   301,   463,  9255,   490, 28706,   613, 28809, 28719,\n",
       "           2590, 15671,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,   320, 12523,  2245, 28747,   802, 28757, 14595, 28735,\n",
       "           3851,   298,  3848,   456,  3154, 28723, 28301,   272, 24942,   773,\n",
       "          28705, 28781, 28734, 28823,   805,   954, 24104, 28723,  7336,  2240,\n",
       "            295,  1449,   460,   459,  4525,   345,   763, 24104, 28739,  4449,\n",
       "           1508, 28707, 28723,  1115, 28748, 28728,   510, 16457, 28740, 28765,\n",
       "           1737, 28787,    13,  4565],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              1,   320, 12523,  2245, 28747,   802, 28719,  3865,   322,   802,\n",
       "          28719,  3810,   444, 28721,  9050,   851,   349,   272,  1080,  7714,\n",
       "           8710,  1970,   315, 28742,   333,  2270,  3364,   302, 28723,  8580,\n",
       "           3707,   349,  1038,  2525,  7656,  4564,  2117, 28878,  4449,  1508,\n",
       "          28707, 28723,  1115, 28748, 28719, 28783, 28715,  2547, 28734,   278,\n",
       "          28779, 28726,    13,  4565]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset[\"test\"].map(\n",
    "    test_preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PEFT model, Optimizer and LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt tunning config\n",
    "# this is the general instruction\n",
    "prompt_tunning_init_text = \"Classify if the tweet is a complaint or no complaint. \\n\"\n",
    "# now peft config\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=len(tokenizer(prompt_tunning_init_text)[\"input_ids\"]),\n",
    "    prompt_tuning_init_text=prompt_tunning_init_text,\n",
    "    tokenizer_name_or_path=model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90abc7fbfe09400396d742e12354a565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fad5be6da9485baae862e771a37775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212ff0ec57914b5aa7e498274ec73358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9303ecd0c4e04a90899f7b2434183d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b967d5293b477ba88cb2e8b4259982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5df3b594574b7e9b95d651cf2ce86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35253d44a35142169fb4680461daf8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 65,536 || all params: 7,241,797,632 || trainable%: 0.0009\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "# now homologate model to peft\n",
    "model = get_peft_model(model, peft_config)\n",
    "# print all trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "# now enable gradient checkpointing for resume if stopped\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters are 0.0009% of total model parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader)*num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation on test samples before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1685: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet text: @TommyHilfiger Dramatic shopping exp. ordered 6 jeans same size (30/32) 2 fits / 2 too large / 2 too slim : same brand &gt; different sizing\n",
      "Label : 30/32\n",
      "\n",
      "I’ve been a fan of Tommy Hilfiger for a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 33\n",
    "inputs = tokenizer(\n",
    "    f'{text_column}: {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basemodel does not perform as expected, cause in example `label: 30/32` does not seem to represent an intuitive label at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "100%|██████████| 7/7 [00:02<00:00,  3.01it/s]\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(31.5284, device='cuda:0') train_epoch_loss=tensor(3.4509, device='cuda:0') eval_ppl=tensor(8.2772, device='cuda:0') eval_epoch_loss=tensor(2.1135, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.05it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(4.7345, device='cuda:0') train_epoch_loss=tensor(1.5549, device='cuda:0') eval_ppl=tensor(2.8114, device='cuda:0') eval_epoch_loss=tensor(1.0337, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.09it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(2.1326, device='cuda:0') train_epoch_loss=tensor(0.7573, device='cuda:0') eval_ppl=tensor(1.5704, device='cuda:0') eval_epoch_loss=tensor(0.4513, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.09it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(1.3959, device='cuda:0') train_epoch_loss=tensor(0.3335, device='cuda:0') eval_ppl=tensor(1.3479, device='cuda:0') eval_epoch_loss=tensor(0.2985, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.86it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(1.2944, device='cuda:0') train_epoch_loss=tensor(0.2580, device='cuda:0') eval_ppl=tensor(1.1923, device='cuda:0') eval_epoch_loss=tensor(0.1759, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=tensor(1.1660, device='cuda:0') train_epoch_loss=tensor(0.1536, device='cuda:0') eval_ppl=tensor(1.1180, device='cuda:0') eval_epoch_loss=tensor(0.1115, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=tensor(1.0908, device='cuda:0') train_epoch_loss=tensor(0.0869, device='cuda:0') eval_ppl=tensor(1.0745, device='cuda:0') eval_epoch_loss=tensor(0.0718, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.08it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl=tensor(1.0732, device='cuda:0') train_epoch_loss=tensor(0.0706, device='cuda:0') eval_ppl=tensor(1.0548, device='cuda:0') eval_epoch_loss=tensor(0.0534, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  3.08it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl=tensor(1.0502, device='cuda:0') train_epoch_loss=tensor(0.0490, device='cuda:0') eval_ppl=tensor(1.0473, device='cuda:0') eval_epoch_loss=tensor(0.0462, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:02<00:00,  2.89it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl=tensor(1.0450, device='cuda:0') train_epoch_loss=tensor(0.0440, device='cuda:0') eval_ppl=tensor(1.0449, device='cuda:0') eval_epoch_loss=tensor(0.0439, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    # state model on train stage\n",
    "    model.train()\n",
    "    # this will count loss over each epoch\n",
    "    total_loss = 0\n",
    "    # start iterating over each batch on train dataloader\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # send batch values to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.autocast(dtype=torch.float16, device_type=\"cuda\"):\n",
    "            # map batch values to model and get outputs on forward pass\n",
    "            outputs = model(**batch)\n",
    "        # get loss from output\n",
    "        loss = outputs.loss\n",
    "        # start accumulating losses over each epoch\n",
    "        total_loss += loss.detach().float()\n",
    "        # do backward step\n",
    "        loss.backward()\n",
    "        # optimize on backward step\n",
    "        optimizer.step()\n",
    "        # define if learning rate must be redefine on this step\n",
    "        lr_scheduler.step()\n",
    "        # reset gradients for next step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # state model on evaluation stage\n",
    "    model.eval()\n",
    "    # this will count loss over each epoch for evaluation set\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    # start iterating over each batch on evaluation dataloader\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        # send batch values to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            # use no gradients for this cause this is eval only\n",
    "            # map batch values to model and get outputs\n",
    "            outputs = model(**batch)\n",
    "        # get loss from evaluation outputs\n",
    "        loss = outputs.loss\n",
    "        # start accumulating losses over each epoch\n",
    "        eval_loss += loss.detach().float()\n",
    "        # storage predictions\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # get average of eval losses\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    # get perpelexity of eval losses\n",
    "    # perplexity: measure of how well a model is accomplishing its task.\n",
    "    # read further info here: https://developers.google.com/machine-learning/glossary#perplexity\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    # get average of train losses\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    # get perpelexity of train losses\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "    # log metrics on wandb\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train\": {\"perplexity\": train_ppl, \"loss\": train_epoch_loss, \"epoch\": epoch},\n",
    "            \"val\": {\"perplexity\": eval_ppl, \"loss\": eval_epoch_loss, \"epoch\": epoch} \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation on test samples after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1685: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Tweet text: @TommyHilfiger Dramatic shopping exp. ordered 6 jeans same size (30/32) 2 fits / 2 too large / 2 too slim : same brand &gt; different sizing\n",
      "Label: 2015-\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 33\n",
    "inputs = tokenizer(\n",
    "    f'{text_column}: {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel: ',\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=target_max_length+1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well label says 2015 which is not a label class of the defined in the dataset :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guide pushes the model to huggingface hub and then load it again to evaluate on test samples. </br>\n",
    "```python\n",
    "model.push_to_hub(\n",
    "    f\"mistral_prompt_tuning\",\n",
    "    token=\"hf_...\",\n",
    "    private=True\n",
    ")\n",
    "```\n",
    "\n",
    "Or save locally:\n",
    "```python\n",
    "peft_model_id = \"mistral_prompt_tuning\",\n",
    "model.save_pretrained(peft_model_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unlabeled', 'complaint', 'no complaint']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In any case, the model checkpoint is extremelly much smaller in size than original model, which improves a lot the deployment of this model in downstream tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
