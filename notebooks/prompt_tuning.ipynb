{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to login to wandb first using the cli:\n",
    "```\n",
    "wandb login\n",
    "<paste wandb api key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='prompt_learning_methods', name='prompt_tuning')\n",
    "seed = 0\n",
    "device = 'cuda'\n",
    "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "dataset_name = \"twitter_complaints\"\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "print(classes)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dataset[\"train\"][\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
    "print(f\"{target_max_length=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x}\\nLabel\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(input) \n",
    "    labels = tokenizer(targets, add_special_tokens=False)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (max_length - len(sample_input_ids)) + model_inputs[\"attention_mask\"][i]\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][:max_length])\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")\n",
    "\n",
    "train_dataset = preprocessed_dataset[\"train\"]\n",
    "eval_dataset = preprocessed_dataset[\"train\"]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column}: {x}\\nLabel\" for x in examples[text_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id]*(\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"test\"].map(\n",
    "    test_preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PEFT model, Optimizer and LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt tunning config\n",
    "# this is the general instruction\n",
    "prompt_tunning_init_text = \"Classify if the tweet is a complaint or no complaint. \\n\"\n",
    "# now peft config\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=len(tokenizer(prompt_tunning_init_text)[\"input_ids\"]),\n",
    "    prompt_tuning_init_text=prompt_tunning_init_text,\n",
    "    tokenizer_name_or_path=model_name_or_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "# now homologate model to peft\n",
    "model = get_peft_model(model, peft_config)\n",
    "# print all trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "# now enable gradient checkpointing for resume if stopped\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrenant\": False})\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader)*num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation on test samples before finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 33\n",
    "inputs = tokenizer(\n",
    "    f'{text_column}: {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel : ',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        atytention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    # state model on train stage\n",
    "    model.train()\n",
    "    # this will count loss over each epoch\n",
    "    total_loss = 0\n",
    "    # start iterating over each batch on train dataloader\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # send batch values to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.autocast(detype=torch.float16, device_type=\"cuda\"):\n",
    "            # map batch values to model and get outputs on forward pass\n",
    "            outputs = model(**batch)\n",
    "        # get loss from output\n",
    "        loss = outputs.loss\n",
    "        # start accumulating losses over each epoch\n",
    "        total_loss += loss.detach().float()\n",
    "        # do backward step\n",
    "        loss.backward()\n",
    "        # optimize on backward step\n",
    "        optimizer.step()\n",
    "        # define if learning rate must be redefine on this step\n",
    "        lr_scheduler.step()\n",
    "        # reset gradients for next step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # state model on evaluation stage\n",
    "    model.eval()\n",
    "    # this will count loss over each epoch for evaluation set\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    # start iterating over each batch on evaluation dataloader\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        # send batch values to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            # use no gradients for this cause this is eval only\n",
    "            # map batch values to model and get outputs\n",
    "            outputs = model(**batch)\n",
    "        # get loss from evaluation outputs\n",
    "        loss = outputs.loss\n",
    "        # start accumulating losses over each epoch\n",
    "        eval_loss += loss\n",
    "        # storage predictions\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # get average of eval losses\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    # get perpelexity of eval losses\n",
    "    # perplexity: measure of how well a model is accomplishing its task.\n",
    "    # read further info here: https://developers.google.com/machine-learning/glossary#perplexity\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    # get average of train losses\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    # get perpelexity of train losses\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "    # log metrics on wandb\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train\": {\"perplexity\": train_ppl, \"loss\": train_epoch_loss, \"epoch\": epoch},\n",
    "            \"val\": {\"perplexity\": eval_ppl, \"loss\": eval_epoch_loss, \"epoch\": epoch} \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation on test samples after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 33\n",
    "inputs = tokenizer(\n",
    "    f'{text_column}: {dataset[\"test\"][i][\"Tweet text\"]}\\nLabel: ',\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=5,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guide pushes the model to huggingface hub and then load it again to evaluate on test samples. </br>\n",
    "```python\n",
    "model.push_to_hub(\n",
    "    f\"mistral_prompt_tuning\",\n",
    "    token=\"hf_...\",\n",
    "    private=True\n",
    ")\n",
    "```\n",
    "\n",
    "Or save locally:\n",
    "```python\n",
    "peft_model_id = \"mistral_prompt_tuning\",\n",
    "model.save_pretrained(peft_model_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In any case, the model checkpoint is extremelly much smaller in size than original model, which improves a lot the deployment of this model in downstream tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
