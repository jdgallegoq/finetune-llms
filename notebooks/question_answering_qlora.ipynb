{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA applied to question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, default_data_collator, get_linear_schedule_with_warmup, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PromptTuningConfig, PromptTuningInit\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"qlora_fingpt_finqa\"\n",
    "seed = 202408\n",
    "set_seed(seed)\n",
    "\n",
    "# huggingface endoints\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"google/gemma-2-2b\"\n",
    "tokenizer_name_or_path = \"google/gemma-2-2b\"\n",
    "dataset_name = \"FinGPT/fingpt-fiqa_qa\"\n",
    "\n",
    "# Dataset\n",
    "text_column = \"input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Can increasing my tax withholding from my full-time job cover FICA taxes for my freelance work?', 'output': \"Technically you owe 'self-employment' taxes not FICA taxes because they are imposed under a different law, SECA. However, since SE taxes are by design exactly the same rates as combining the two halves of FICA (employer and employee) it is quite reasonable to treat them as equivalent. SE taxes (and income tax also) are based on your net self-employment income, after deducting business expenses (but not non-business items like your home mortgage, dependent exemptions, etc which factor only into income tax). You owe SE Medicare tax 2.9% on all your SE net income (unless it is under $400) adjusted down by 7.65% to compensate for the fact that the employer half of FICA is excluded from gross income before the employee half is computed. You owe SE Social Security tax 12.4% on your adjusted SE net income unless and until the total income subject to FICA+SECA, i.e. your W-2 wages plus your adjusted SE net income, exceeds a cap that varies with inflation and is $127,200 for 2017. OTOH if FICA+SECA income exceeds $200k single or $250k joint you owe Additional Medicare tax 0.9% on the excess; if your W-2 income (alone) exceeds this limit your employer should withhold for it. However the Additional Medicare tax is part of 'Obamacare' (PPACA) which the new President and Republican majorities have said they will 'repeal and replace'; whether any such replacement will affect this for TY 2017 is at best uncertain at this point. Yes SE taxes are added to income tax on your 1040 with schedule SE attached (and schedule C/CEZ, E, F as applicable to your business) (virtually so if you file electronically) and paid together. You are supposed to pay at least 90% during the year by having withholding increased on your W-2 job, or by making 'quarterly' estimated payments (IRS quarters are not exactly quarters, but close), or any combination. But if this is your first year (which you don't say, but someone who had gone through this before probably wouldn't ask) you may get away with not paying during the year as normally required; specifically, if your W-2 withholding is not enough to cover your increased taxes for this year (because of the additional income and SE taxes) but it is enough to cover your tax for the previous year and your AGI that year wasn't over $150k, then there is a 'safe harbor' and you won't owe any form-2210 penalty -- although you must keep enough money on hand to pay the tax by April 15. But for your second year and onwards, your previous year now includes SE amounts and this doesn't help. Similar/related:\", 'instruction': 'Offer your thoughts or opinion on the input financial query or topic using your financial background.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name)\n",
    "# get a sample of dataset to accelerate training\n",
    "#dataset = dataset[\"train\"].train_test_split(train_size=0.1)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on output text lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_lengths = [len(sample[\"output\"]) for sample in dataset[\"train\"]]\n",
    "plt.hist(text_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dataset\n",
    "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "tokenizer.chat_template = template\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target max length =  1029\n"
     ]
    }
   ],
   "source": [
    "# model hyperparams\n",
    "max_length = int(np.average(text_lengths))\n",
    "target_max_length = max_length\n",
    "print(f\"target max length = \", target_max_length)\n",
    "lr = 1e-4\n",
    "num_epochs = 5\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Offer your thoughts or opinion on the input financial query or topic using your financial background.\n",
      "input: [Can increasing my tax withholding from my full-time job cover FICA taxes for my freelance work?]\n",
      "output: Technically you owe 'self-employment' taxes not FICA taxes because they are imposed under a different law, SECA. However, since SE taxes are by design exactly the same rates as combining the two halves of FICA (employer and employee) it is quite reasonable to treat them as equivalent. SE taxes (and income tax also) are based on your net self-employment income, after deducting business expenses (but not non-business items like your home mortgage, dependent exemptions, etc which factor only into income tax). You owe SE Medicare tax 2.9% on all your SE net income (unless it is under $400) adjusted down by 7.65% to compensate for the fact that the employer half of FICA is excluded from gross income before the employee half is computed. You owe SE Social Security tax 12.4% on your adjusted SE net income unless and until the total income subject to FICA+SECA, i.e. your W-2 wages plus your adjusted SE net income, exceeds a cap that varies with inflation and is $127,200 for 2017. OTOH if FICA+SECA income exceeds $200k single or $250k joint you owe Additional Medicare tax 0.9% on the excess; if your W-2 income (alone) exceeds this limit your employer should withhold for it. However the Additional Medicare tax is part of 'Obamacare' (PPACA) which the new President and Republican majorities have said they will 'repeal and replace'; whether any such replacement will affect this for TY 2017 is at best uncertain at this point. Yes SE taxes are added to income tax on your 1040 with schedule SE attached (and schedule C/CEZ, E, F as applicable to your business) (virtually so if you file electronically) and paid together. You are supposed to pay at least 90% during the year by having withholding increased on your W-2 job, or by making 'quarterly' estimated payments (IRS quarters are not exactly quarters, but close), or any combination. But if this is your first year (which you don't say, but someone who had gone through this before probably wouldn't ask) you may get away with not paying during the year as normally required; specifically, if your W-2 withholding is not enough to cover your increased taxes for this year (because of the additional income and SE taxes) but it is enough to cover your tax for the previous year and your AGI that year wasn't over $150k, then there is a 'safe harbor' and you won't owe any form-2210 penalty -- although you must keep enough money on hand to pay the tax by April 15. But for your second year and onwards, your previous year now includes SE amounts and this doesn't help. Similar/related:<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt, input, output = dataset[\"train\"][0][\"instruction\"], dataset[\"train\"][0][\"input\"], dataset[\"train\"][0][\"output\"]\n",
    "input_desc_formatted = f\"[{input}]\"\n",
    "system_message = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": f\"{system_prompt}\\ninput: {input_desc_formatted}\\noutput: {output}\"\n",
    "    }\n",
    "print(tokenizer.apply_chat_template([system_message], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(samples):\n",
    "    batch = []\n",
    "    for system_prompt, input, output in zip(samples[\"instruction\"], samples[\"input\"], samples[\"output\"]):\n",
    "        try:\n",
    "            input_desc_formatted = json.dumps(json.loads(f\"[{input}]\", indent=2, sort_keys=True))\n",
    "        except:\n",
    "            input_desc_formatted = f\"[{input}]\"\n",
    "        system_message = {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"{system_prompt}\\ninput: {input_desc_formatted}\\noutput: {output}\"\n",
    "        }\n",
    "        batch.append(tokenizer.apply_chat_template([system_message], tokenize=False))\n",
    "    return {\"content\": batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"<|im_start|>system\\nUtilize your financial knowledge, give your answer or opinion to the input question or subject . Answer format is not limited.\\ninput: [Rolled over husband's 401(k) to IRA after his death. Can I deduct a loss since?]\\noutput: I trust the 401(k) was a traditional, pre tax account. There was no tax paid, and any withdrawals would be taxable. The account could go to zero, and there's no write off, sorry. I have to ask - were there any withdrawals along the way? What was it invested in that lost 90% of its value?  Edit - I'm sorry the OP came and went. It would be great to have closure on some of these issues. Here, I'm thinking as Duff said, malpractice, or perhaps a 401(k) that was 100% in company stock. Seems we'll never know.<|im_end|>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# tokenize and preprocess dataset to be\n",
    "# readable by the model as model inputs\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test_preprocess_function(examples):\n",
    "#    batch_size = len(examples[text_column])\n",
    "#    inputs = [f\"{text_column}: {x}\\noutput\" for x in examples[text_column]]\n",
    "#    model_inputs = tokenizer(inputs)\n",
    "#    for i in range(batch_size):\n",
    "#        sample_inputs_ids = model_inputs[\"input_ids\"][i]\n",
    "#        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id]*(\n",
    "#            max_length - len(sample_inputs_ids)\n",
    "#        ) + sample_inputs_ids\n",
    "#        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_inputs_ids)) + \\\n",
    "#            model_inputs[\"attention_mask\"][i]\n",
    "#        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "#        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "#    \n",
    "#    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this instruction is from the dataset\n",
    "#prompt_tunning_init_text = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}. \\n\"\n",
    "target_modules = [\n",
    "    \"gate_proj\",\n",
    "    \"q_proj\",\n",
    "    \"lm_head\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"embed_tokens\",\n",
    "    \"down_proj\",\n",
    "    \"up_proj\",\n",
    "    \"v_proj\"\n",
    "]\n",
    "\n",
    "# now create peft config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "# bits and bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # normal bytes 4\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True # this helps saving GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    function_call = \"<|im_start|>function-call\"\n",
    "    function_response = \"<|im_start|>function-response\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732001468dd84df7a781916633aa2218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(256008, 2304, padding_idx=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    bos_token=ChatmlSpecialTokens.bos_token.value,\n",
    "    eos_token=ChatmlSpecialTokens.eos_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.chat_template = template\n",
    "\n",
    "# create model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"<|im_start|>system\\nUtilize your financial knowledge, give your answer or opinion to the input question or subject . Answer format is not limited.\\ninput: [Rolled over husband's 401(k) to IRA after his death. Can I deduct a loss since?]\\noutput: I trust the 401(k) was a traditional, pre tax account. There was no tax paid, and any withdrawals would be taxable. The account could go to zero, and there's no write off, sorry. I have to ask - were there any withdrawals along the way? What was it invested in that lost 90% of its value?  Edit - I'm sorry the OP came and went. It would be great to have closure on some of these issues. Here, I'm thinking as Duff said, malpractice, or perhaps a 401(k) that was 100% in company stock. Seems we'll never know.<|im_end|>\\n\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': packing, dataset_text_field, max_seq_length, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:366: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"gemma2b_fingptqa\"\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "logging_steps = 5\n",
    "learning_rate = 5e-4\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 2048\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    bf16=True, # set to True for quantization usage\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"content\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    peft_config=peft_config,\n",
    "    dataset_kwargs={\n",
    "        \"append_concat_token\": False,\n",
    "        \"add_special_tokens\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjdgallegoq\u001b[0m (\u001b[33mjdgallegoq-wandb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/finetune-llms/notebooks/wandb/run-20240823_191840-3a2n3tnx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jdgallegoq-wandb/qlora_fingpt_finqa/runs/3a2n3tnx' target=\"_blank\">gemma2b_fingptqa</a></strong> to <a href='https://wandb.ai/jdgallegoq-wandb/qlora_fingpt_finqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jdgallegoq-wandb/qlora_fingpt_finqa' target=\"_blank\">https://wandb.ai/jdgallegoq-wandb/qlora_fingpt_finqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jdgallegoq-wandb/qlora_fingpt_finqa/runs/3a2n3tnx' target=\"_blank\">https://wandb.ai/jdgallegoq-wandb/qlora_fingpt_finqa/runs/3a2n3tnx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `flash_attention_2`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='259' max='259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [259/259 31:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.204300</td>\n",
       "      <td>2.196367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea56de6765544ba9bfcd4e87fa71f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f2b331014740f9abe1427ed452794e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9521343cbc411d934d1b161f509b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1724440718.2ec384b33f14:   0%|          | 0.00/17.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e8a885375d452eafd9ac34591d7323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32deae82c1f4d0ca559f8c407817a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c911e89c21b148e1bcf0c227f37f2047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bc8cd6e67d42c8974a62bc026178bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "peft_model_id = \"jdgallegoq96/gemma2b_fingptqa\"\n",
    "device = \"cuda\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index put requires the source and destination dtypes match, got Half for the destination and Float for the source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, device_type\u001b[38;5;241m=\u001b[39mdevice):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1029\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py:1638\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1637\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1638\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1640\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:994\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    992\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1008\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:843\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    832\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    833\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    834\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m         cache_position,\n\u001b[1;32m    841\u001b[0m     )\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:586\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    596\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma2/modeling_gemma2.py:380\u001b[0m, in \u001b[0;36mGemma2FlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin,\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos,\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    379\u001b[0m     }\n\u001b[0;32m--> 380\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:1532\u001b[0m, in \u001b[0;36mHybridCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     update_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_update\n\u001b[0;32m-> 1532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py:1496\u001b[0m, in \u001b[0;36mHybridCache._sliding_update\u001b[0;34m(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len)\u001b[0m\n\u001b[1;32m   1493\u001b[0m k_out \u001b[38;5;241m=\u001b[39m k_out[:, :, indices]\n\u001b[1;32m   1494\u001b[0m v_out \u001b[38;5;241m=\u001b[39m v_out[:, :, indices]\n\u001b[0;32m-> 1496\u001b[0m \u001b[43mk_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m key_states\n\u001b[1;32m   1497\u001b[0m v_out[:, :, cache_position] \u001b[38;5;241m=\u001b[39m value_states\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index put requires the source and destination dtypes match, got Half for the destination and Float for the source."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "i = 123\n",
    "#text = tokenizer.apply_chat_template(dataset[\"test\"][i], tokenize=False)\n",
    "inputs = tokenizer(dataset[\"test\"][i][\"content\"], return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "with torch.autocast(dtype=torch.bfloat16, device_type=device):\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1029,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.0,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
