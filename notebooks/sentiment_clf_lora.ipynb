{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA applied to sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PromptTuningConfig, PromptTuningInit\n",
    "from trl import SFTTrainer\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"lora_fingpt_sentiment_clf\"\n",
    "seed = 202408\n",
    "set_seed(seed)\n",
    "\n",
    "# huggingface endoints\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "dataset_name = \"FinGPT/fingpt-sentiment-train\"\n",
    "\n",
    "# Dataset\n",
    "text_column = \"input\"\n",
    "label_column = \"output\"\n",
    "\n",
    "# model hyperparams\n",
    "max_length = 64\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive', 'mildly negative', 'moderately positive', 'strong negative', 'moderately negative', 'negative', 'neutral', 'strong positive', 'mildly positive'}\n",
      "{'input': 'Teollisuuden Voima Oyj , the Finnish utility known as TVO , said it shortlisted Mitsubishi Heavy s EU-APWR model along with reactors from Areva , Toshiba Corp. , GE Hitachi Nuclear Energy and Korea Hydro & Nuclear Power Co. .', 'output': 'neutral', 'instruction': 'What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name)\n",
    "classes = [row[label_column] for row in dataset[\"train\"]]\n",
    "print(set(classes))\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral', 'negative', 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# instruction only states 3 possible labels {negative/neutral/positive}\n",
    "# I assume available labels need to be enforced to be one of these 3\n",
    "\n",
    "# define a dict to map labels\n",
    "classes_dict = {}\n",
    "for c in classes:\n",
    "    if 'positive' in c:\n",
    "        classes_dict[c] = 'positive'\n",
    "    elif 'negative' in c:\n",
    "        classes_dict[c] = 'negative'\n",
    "    else:\n",
    "        classes_dict[c] = 'neutral'\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes_dict[label] for label in x[label_column]]},\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")\n",
    "print(set([row['text_label'] for row in dataset[\"train\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 29215, 'negative': 17047, 'positive': 30510})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count labels proportions\n",
    "Counter(dataset[\"train\"][\"text_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/transformers/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target max length =  4\n"
     ]
    }
   ],
   "source": [
    "# preprocess dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes_dict.keys()])\n",
    "print(f\"target max length = \", target_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x}\\nLabel\" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[\"text_label\"]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets, add_special_tokens=False) # dont add bos token cause this is concatenated with input\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.eos_token_id]\n",
    "        #print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + \\\n",
    "            model_inputs[\"attention_mask\"][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 76772/76772 [00:05<00:00, 13990.39 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     1,  2787,   714,\n",
       "           4515, 13706, 28747,   429,  3477,  5550, 10203,   271,   456,  3970,\n",
       "            553,   706, 26082,   805,  1500, 27536,  5028, 28725,  5756,   847,\n",
       "            507, 28713,   582,   544,  1370,   298,  2848,   356,  1486, 28713,\n",
       "             13,  4565,  5278,     2],\n",
       "         [    2,     2,     2,     2,     1,  2787,   714,   429, 28738,  8923,\n",
       "          28796,   387,   365,   834, 16027, 28725,  5671, 28723,   325, 28738,\n",
       "           8923, 28796, 28731, 13117,  8474,   753,   524,  3711,  9381,  5253,\n",
       "            356,  1186, 28781, 28705, 28750, 28734, 28740, 28774, 17412,   387,\n",
       "            413,  1331,   742,  5529,  4335,  2489,  3406,  4449,  1508, 28707,\n",
       "          28723,  1115, 28748,  1017, 28711, 28790,  6158, 28712,  9269, 28759,\n",
       "             13,  4565, 14214,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "           2787,   714,   320, 28169,   324,  5945,  1200,   264,  9652,   302,\n",
       "            524,   366,  4807,  2071,  1200, 13186,   684, 28705, 28750, 28770,\n",
       "           1239,   302,   272,  8098,  2668,   297,   871,  1834,   304, 25061,\n",
       "            662, 28723, 17658,  5467,  6192, 13876,   320,  2335, 28723,   842,\n",
       "             13,  4565, 14214,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "           2787,   714,   415,  2496,   464, 28713,  9558,  4320,  3558,   286,\n",
       "            298,   413, 28749, 28796, 28705, 28740, 28783, 15358,  1200,  6731,\n",
       "            298,   264, 11876,   302,   413, 28749, 28796, 28705, 28787, 28723,\n",
       "          28770,   290, 28711,   297, 28705, 28750, 28734, 28734, 28783,   842,\n",
       "             13,  4565,  7087,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,  2787,   714,   415,  8966,   349,   297,  1407,   395,\n",
       "            330,   886, 28709,   464, 28713,  7213,   298,  6710,   297,   272,\n",
       "          17111,   294, 11759,  4424,   304,   297,   272, 16495, 11574,   842,\n",
       "             13,  4565,  5278,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     1,\n",
       "           2787,   714,   475,   383,   515,   298,  7303, 10818, 28185, 24391,\n",
       "            304, 11439,  7601, 28705, 28750, 28734, 28740, 28774, 17412,   356,\n",
       "           5353, 28705, 28750, 28782, 28725, 28705, 28750, 28734, 28750, 28734,\n",
       "             13,  4565, 14214,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,  2787,   714,   320,   563, 15874,  6400,   298,  3119,\n",
       "            582,   298, 28705, 28770, 28725, 28734, 28734, 28734,  6392,  7794,\n",
       "             13,  4565, 14214,     2],\n",
       "         [    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     1,  2787,   714,  9117,   286, 22461,   506, 20135,  7153,\n",
       "          12896,   354,   741,   727, 28723,  1725,   993,   347,  8634, 28723,\n",
       "             13,  4565, 14214,     2]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  5278,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 14214,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 14214,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  7087,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  5278,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 14214,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 14214,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100, 14214,     2]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and preprocess dataset to be\n",
    "# readable by the model as model inputs\n",
    "preprocessed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\"\n",
    ")\n",
    "\n",
    "# now get datasets\n",
    "train_dataset = preprocessed_dataset[\"train\"]\n",
    "eval_dataset = preprocessed_dataset[\"train\"] # there's no eval on the dataset\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True\n",
    ")\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test_preprocess_function(examples):\n",
    "#    batch_size = len(examples[text_column])\n",
    "#    inputs = [f\"{text_column}: {x}\\nLabel\" for x in examples[text_column]]\n",
    "#    model_inputs = tokenizer(inputs)\n",
    "#    for i in range(batch_size):\n",
    "#        sample_inputs_ids = model_inputs[\"input_ids\"][i]\n",
    "#        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id]*(\n",
    "#            max_length - len(sample_inputs_ids)\n",
    "#        ) + sample_inputs_ids\n",
    "#        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_inputs_ids)) + \\\n",
    "#            model_inputs[\"attention_mask\"][i]\n",
    "#        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "#        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "#    \n",
    "#    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this instruction is from the dataset\n",
    "#prompt_tunning_init_text = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}. \\n\"\n",
    "target_modules = [\n",
    "    \"gate_proj\",\n",
    "    \"q_proj\",\n",
    "    \"lm_head\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"embed_tokens\",\n",
    "    \"down_proj\",\n",
    "    \"up_proj\",\n",
    "    \"v_proj\"\n",
    "]\n",
    "\n",
    "# now create peft config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = get_peft_model(model, peft_config)\n",
    "# see trainable params\n",
    "model.print_trainable_parameters()\n",
    "# cast non-trainable parameters to fp16\n",
    "for p in model.parameters():\n",
    "    if not p.requires_grad:\n",
    "        p.data = p.to(torch.float16)\n",
    "# enable model gradient checkpointing\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "# pass model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader)*num_epochs)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "for epoch in range(num_epochs):\n",
    "    # state model on train stage\n",
    "    model.train()\n",
    "    # this will count loss over each epoch\n",
    "    total_loss = 0\n",
    "    # start iterating over each batch on train dataloader\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # send batch values to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.autocast(dtype=torch.float16, device_type=\"cuda\"):\n",
    "            # map batch values to model and get outputs on forward pass\n",
    "            outputs = model(**batch)\n",
    "        # get loss from output\n",
    "        loss = outputs.loss\n",
    "        # start accumulating losses over each epoch\n",
    "        total_loss += loss.detach().float()\n",
    "        # do backward step\n",
    "        loss.backward()\n",
    "        # optimize on backward step\n",
    "        optimizer.step()\n",
    "        # define if learning rate must be redefine on this step\n",
    "        lr_scheduler.step()\n",
    "        # reset gradients for next step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # state model on evaluation stage\n",
    "    model.eval()\n",
    "    # this will count loss over each epoch for evaluation set\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    # start iterating over each batch on evaluation dataloader\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        # send batch values to GPU\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            # use no gradients for this cause this is eval only\n",
    "            # map batch values to model and get outputs\n",
    "            outputs = model(**batch)\n",
    "        # get loss from evaluation outputs\n",
    "        loss = outputs.loss\n",
    "        # start accumulating losses over each epoch\n",
    "        eval_loss += loss.detach().float()\n",
    "        # storage predictions\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(\n",
    "                torch.argmax(outputs.logits, -1).detach().cpu().numpy(),\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # get average of eval losses\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    # get perpelexity of eval losses\n",
    "    # perplexity: measure of how well a model is accomplishing its task.\n",
    "    # read further info here: https://developers.google.com/machine-learning/glossary#perplexity\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    # get average of train losses\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    # get perpelexity of train losses\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "    # log metrics on wandb\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train\": {\"perplexity\": train_ppl, \"loss\": train_epoch_loss, \"epoch\": epoch},\n",
    "            \"val\": {\"perplexity\": eval_ppl, \"loss\": eval_epoch_loss, \"epoch\": epoch} \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "i = 2\n",
    "inputs = tokenizer(\n",
    "    f\"{text_column}: {dataset[\"train\"][i][text_column]}\\nLabel: \",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=target_max_length+1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
