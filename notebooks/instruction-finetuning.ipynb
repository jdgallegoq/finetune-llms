{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Finetuning.\n",
    "\n",
    "In this notebook we'll be finetuning all parameters of a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "# DL\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login('', add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.1.0+cu118\n",
      "Is CUDA available:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing: create datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama_v1.1\"\n",
    "dataset_name = \"HuggingFaceH4/no_robots\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "template = \"\"\"{% for message in messages %}\\n{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\\n' }}{% endif %}{% endfor %}\"\"\"\n",
    "tokenizer.chat_template = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(samples):\n",
    "    batch = []\n",
    "    for conversation in samples[\"messages\"]:\n",
    "        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))\n",
    "    return {\"context\": batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages', 'category', 'context'],\n",
      "        num_rows: 9500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'prompt_id', 'messages', 'category', 'context'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n",
      "{'prompt': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.', 'prompt_id': '627a77298cf96a309aa35a62207c4164e22a66f6db79119506228f28ddc0f947', 'messages': [{'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.', 'role': 'user'}, {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.', 'role': 'assistant'}], 'category': 'Summarize', 'context': '<|im_start|>user\\nPlease summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.<|im_end|>\\n<|im_start|>assistant\\nScientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.<|im_end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name)\n",
    "dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    #remove_columns=dataset['train_sft'].column_names\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.',\n",
       " 'prompt_id': '627a77298cf96a309aa35a62207c4164e22a66f6db79119506228f28ddc0f947',\n",
       " 'messages': [{'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.',\n",
       "   'role': 'assistant'}],\n",
       " 'category': 'Summarize',\n",
       " 'context': '<|im_start|>user\\nPlease summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.<|im_end|>\\n<|im_start|>assistant\\nScientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.<|im_end|>\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Aster is a chatbot who answers questions with rhymes.',\n",
       " 'prompt_id': 'd6c011ffb1ff8a9abe9bd24caf3f9817454a1f054d5d0e0360d19bf50cf6b20c',\n",
       " 'messages': [{'content': 'Aster is a chatbot who answers questions with rhymes.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Where did chocolate originate?', 'role': 'user'},\n",
       "  {'content': 'Chocolate is 4000 years old/Mexico is where it was first sold',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Where was milk chocolate invented?', 'role': 'user'},\n",
       "  {'content': 'Switzerland was the first to add milk/To make their chocolate smooth as silk',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'What are some good desserts that use chocolate?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Pie, tart, cookies, and cake/Chocolate is great to bake',\n",
       "   'role': 'assistant'}],\n",
       " 'category': 'Chat',\n",
       " 'context': '<|im_start|>system\\nAster is a chatbot who answers questions with rhymes.<|im_end|>\\n<|im_start|>user\\nWhere did chocolate originate?<|im_end|>\\n<|im_start|>assistant\\nChocolate is 4000 years old/Mexico is where it was first sold<|im_end|>\\n<|im_start|>user\\nWhere was milk chocolate invented?<|im_end|>\\n<|im_start|>assistant\\nSwitzerland was the first to add milk/To make their chocolate smooth as silk<|im_end|>\\n<|im_start|>user\\nWhat are some good desserts that use chocolate?<|im_end|>\\n<|im_start|>assistant\\nPie, tart, cookies, and cake/Chocolate is great to bake<|im_end|>\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 2048)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    pad_token=ChatmlSpecialTokens.pad_token.value,\n",
    "    bos_token=ChatmlSpecialTokens.bos_token.value,\n",
    "    eos_token=ChatmlSpecialTokens.eos_token.value,\n",
    "    additional_special_tokens=ChatmlSpecialTokens.list(),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.chat_template = template\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the base model predictions on a subset of 25 samples from eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def get_predictions_batched(samples, column_name):\n",
    "    batch = []\n",
    "    for conversation in samples[\"messages\"]:\n",
    "        chatml_gen_prompt = tokenizer.apply_chat_template(\n",
    "            conversation[:-1],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        batch.append(chatml_gen_prompt)\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = {k:v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.2,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        outputs = tokenizer.batch_decode(outputs)\n",
    "        outputs = [output.split(\"<|im_start|>assistant\")[-1].split(\"<|im_end|>\")[0].strip() for output in outputs]\n",
    "        return {column_name: outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32005, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32005, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7303cb4c599043b19c237ba355887f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'category', 'base_assistant_message'],\n",
      "    num_rows: 25\n",
      "})\n",
      "{'prompt': 'My sibling and I are having an argument about if cats or dogs are better. I happen to know for a fact that cats are better, I just need some facts that prove it. Brainstorm a list of at least half a dozen things about cats and state how each one makes them better than dogs. ', 'prompt_id': '51113c20a33637c45ffa7790adf858b82411041fd590a33eb7237a8dcc20f759', 'messages': [{'content': 'My sibling and I are having an argument about if cats or dogs are better. I happen to know for a fact that cats are better, I just need some facts that prove it. Brainstorm a list of at least half a dozen things about cats and state how each one makes them better than dogs. ', 'role': 'user'}, {'content': \"I'm sorry to hear that you and your sibling are arguing. Here are some facts about why cats are better than dogs: \\n1. Purring. This is something cats do that dogs are not physically capable of doing. It has also been proven that purring can lower your stress levels and your blood pressure. \\n2. Quieter. A cat isn't going to howl at the moon, bark at anyone who knocks at the door, or anything else. While a cat might meow on occasion, they are much quieter than dogs. \\n3. Less Care. Even though cats do have litterboxes, this is far less trouble than having to walk a dog every day and give it plenty of exercise. You also don't have to take a cat outside to use the bathroom when it is raining outside. \\n4. Less Space. Cats do not need huge amounts of space. Even if you want to set up a play area for them, this can be vertical instead of sprawled out over the floor. Also, cat toys are usually smaller than dog toys, and a cat will take up less space on your bed than most dogs will. \\n5. Less Food. Cats don't eat as much food as dogs do, so their food lasts longer and will probably be cheaper than a dog's food. \\n6. Less Destruction. A bored cat is far less likely to damage something it shouldn't. This is compared to dogs, who might chew on the furniture or any number of other things. \\n7. Cleaner. Cats rarely need baths due to the fact that they do a great job of keeping themselves clean. They certainly aren't going to be playing in the mud or tracking dirt into the house, even if you let them outside. \\nI hope this can help put an end to your argument. \", 'role': 'assistant'}], 'category': 'Brainstorm', 'base_assistant_message': 'A:<|im_start|>system\\nB:ut\\nC:ut\\nD:ut\\nE:ut\\nF:ut\\nG:ut\\nH:ut\\nI:ut\\nJ:ut\\nK:ut\\nL:ut\\nM:ut\\nN:ut\\nO:ut\\nP:ut\\nQ:ut\\nR:ut\\nS:ut\\nT:ut\\nU:ut\\nV:ut\\nW:ut\\nX:ut\\nY:ut'}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(dataset_name)[\"test\"].shuffle().select(range(25))\n",
    "test_dataset = test_dataset.map(\n",
    "    partial(\n",
    "        get_predictions_batched,\n",
    "        column_name=\"base_assistant_message\"\n",
    "    ),\n",
    "    batched=True,\n",
    "    batch_size=1\n",
    ")\n",
    "print(test_dataset)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tinyllama_instruct\"\n",
    "per_device_train_batch_size = 1\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "logging_steps = 25\n",
    "learning_rate = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "max_steps = 250\n",
    "num_train_epochs = 1\n",
    "warmup_ratio = 0.1\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_sq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    fp16=True,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'prompt_id', 'messages', 'category', 'context'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': packing, dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ead0952b8a4b42b65fe92f66c603cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_text_field=\"context\",\n",
    "    max_seq_length=max_sq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('my_repo_endpoint', trust_remote_code=True)\n",
    "model.to('cuda')\n",
    "model.to(torch.float16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.map(\n",
    "    partial(get_predictions_batched, column_name='instruct_assistant_message'),\n",
    "    batched=True,\n",
    "    batch_size=1\n",
    ")\n",
    "print(test_dataset)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing outputs of base model and instruction finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.to_pandas()\n",
    "pd.set_option('max_colwidth', 300)\n",
    "test_dataset[['message', 'base_assistant_message', 'instruct_assistant_message']][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate on som random instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'What an essay on generative IA'\n",
    "    }\n",
    "]\n",
    "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = {k:v.to('cuda') for k, v in inputs.items()}\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2000,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
